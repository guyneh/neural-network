{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST dataset of digit recognition\n",
    "\n",
    "Tutorial: https://www.geeksforgeeks.org/handwritten-digit-recognition-using-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tkinter import *\n",
    "from PIL import ImageGrab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "normalisation_factor = 1/255        # Each pixel = 8-bit integer (0-255)\n",
    "input_layer_size = 28 * 28          # Number of features (pixels)\n",
    "hidden_layer_size = 100             # Number of hidden units\n",
    "num_labels = 10                     # Number of labels (0-9)\n",
    "maxiter = 100                       # Maximum number of iterations for optimisation\n",
    "lambda_reg = 0.1                    # Regularisation parameter (prevents overfitting)\n",
    "epsilon = 0.15                      # Random initialisation parameter (prevents symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  dict_keys(['__header__', '__version__', '__globals__', 'mldata_descr_ordering', 'data', 'label'])\n",
      "Dataset shape:  (784, 70000)\n"
     ]
    }
   ],
   "source": [
    "# Load mat file of data\n",
    "data = loadmat('../../data/mnist-original.mat')\n",
    "print(\"Keys: \", data.keys())\n",
    "print(\"Dataset shape: \", data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data, X:\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      " (70000, 784)\n",
      "\n",
      "Labels, y:\n",
      "  [0. 0. 0. ... 9. 9. 9.] \n",
      " (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and transpose\n",
    "X = data['data'].T\n",
    "\n",
    "# Normalise the data so that each pixel is in the range [0, 1]\n",
    "X = X * normalisation_factor\n",
    "\n",
    "# Extract labels from data and flatten\n",
    "y = data['label'].flatten()\n",
    "\n",
    "print(\"Data, X:\\n \", X, \"\\n\", X.shape)\n",
    "print(\"\\nLabels, y:\\n \", y, \"\\n\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  (60000, 784) (60000,)\n",
      "Testing size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set with 60,000 samples and test set with 10,000 samples (capital for matrix, lower case for vector)\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000]\n",
    "print(\"Training size: \", X_train.shape, y_train.shape)\n",
    "\n",
    "# (2nd colon specifies all columns)\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:]\n",
    "print(\"Testing size: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample image (random by default)\n",
    "def show_image(X, y, index=None):\n",
    "    # Choose a random index if none is provided\n",
    "    if index is None:\n",
    "        N = X.shape[0]\n",
    "        index = np.random.randint(0, N - 1)\n",
    "    \n",
    "    # Extract image data and label\n",
    "    image = X[index].reshape(28, 28)\n",
    "    label = y[index]\n",
    "    \n",
    "    # Display image and label\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Choose a random index from the total number of images\n",
    "show_image(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1:\n",
      "  [[ 0.0564899   0.09211505 -0.12219238 ...  0.0510903   0.13654108\n",
      "   0.07003471]\n",
      " [ 0.07887657 -0.07975974 -0.14674955 ...  0.1073148  -0.1044658\n",
      "   0.09446192]\n",
      " [ 0.13527415  0.07856832  0.13998605 ... -0.13821143  0.00371639\n",
      "  -0.14945006]\n",
      " ...\n",
      " [ 0.1116916  -0.14701571  0.08794326 ... -0.07956834  0.10622843\n",
      "   0.13081466]\n",
      " [-0.09923961  0.13797239 -0.01115383 ...  0.07839336 -0.09319706\n",
      "   0.03882683]\n",
      " [-0.0344376   0.03816752  0.10267941 ... -0.04504594  0.10591835\n",
      "   0.08282075]] \n",
      " (100, 785)\n",
      "\n",
      "Theta2:\n",
      "  [[ 0.10280817 -0.02398471  0.05700199 ...  0.0672513  -0.10745697\n",
      "  -0.11018158]\n",
      " [-0.05363739  0.04179559  0.12954462 ...  0.04337173  0.05049604\n",
      "   0.12488633]\n",
      " [-0.07819752  0.09825139 -0.11839458 ...  0.08842901  0.13307533\n",
      "   0.09563754]\n",
      " ...\n",
      " [ 0.08054452 -0.0476578   0.07244421 ...  0.05868553 -0.14521927\n",
      "  -0.06042257]\n",
      " [-0.06539018  0.00954774 -0.02286885 ...  0.03781022  0.06449465\n",
      "  -0.14585716]\n",
      " [ 0.04371357 -0.0786583   0.07248684 ... -0.11685788 -0.08884765\n",
      "   0.00191857]] \n",
      " (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly initialise Thetas (weights) between a range of [-epsilon, epsilon]\n",
    "def initialise(a, b, epsilon):\n",
    "    # Scale and shift random values to be within range\n",
    "    c = (np.random.rand(a, b + 1) * (2 * epsilon)) - epsilon\n",
    "    \n",
    "    # Returns matrix of randomly initialised weights, of dimensions a x (b + 1)\n",
    "    return c\n",
    "\n",
    "# epsilon chosen arbitrarily (small enough to avoid saturation and large enough to avoid vanishing gradients)\n",
    "initial_Theta1 = initialise(hidden_layer_size, input_layer_size, epsilon)\n",
    "initial_Theta2 = initialise(num_labels, hidden_layer_size, epsilon)\n",
    "print(\"Theta1:\\n \", initial_Theta1, \"\\n\", initial_Theta1.shape)\n",
    "print(\"\\nTheta2:\\n \", initial_Theta2, \"\\n\", initial_Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters:\n",
      "  [ 0.0564899   0.09211505 -0.12219238 ... -0.11685788 -0.08884765\n",
      "  0.00191857] \n",
      " (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Unroll (combine) the weight matrices into a single column vector (easier for optimisation algorithm)\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "print(\"Initial parameters:\\n \", initial_nn_params, \"\\n\", initial_nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why 79,510?\n",
    "`Theta1`\n",
    "-   hidden_layer_size * (input_layer_size + 1)\n",
    "-   100 * (784 + 1)\n",
    "-   78,500\n",
    "\n",
    "`Theta2`\n",
    "-   num_labels * (hidden_layer_size + 1)\n",
    "-   10 * (100 + 1)\n",
    "-   1,010\n",
    "\n",
    "`TOTAL`\n",
    "-   79,510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of how activation functions affect the matrix: \n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Sigmoid:\n",
      " [[0.26894142 0.5        0.73105858]\n",
      " [0.88079708 0.11920292 0.95257413]]\n",
      "\n",
      "Tanh:\n",
      " [[-0.76159416  0.          0.76159416]\n",
      " [ 0.96402758 -0.96402758  0.99505475]]\n",
      "\n",
      "ReLU:\n",
      " [[0 0 1]\n",
      " [2 0 3]]\n",
      "\n",
      "Leaky ReLU:\n",
      " [[-0.01  0.    1.  ]\n",
      " [ 2.   -0.02  3.  ]]\n",
      "\n",
      "Softmax:\n",
      " [[0.09003057 0.24472847 0.66524096]\n",
      " [0.26762315 0.00490169 0.72747516]]\n",
      "\n",
      "Linear:\n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Softplus:\n",
      " [[0.31326169 0.69314718 1.31326169]\n",
      " [2.12692801 0.12692801 3.04858735]]\n",
      "\n",
      "Hard Sigmoid:\n",
      " [[0.  0.5 1. ]\n",
      " [1.  0.  1. ]]\n",
      "\n",
      "Hard Tanh:\n",
      " [[-1  0  1]\n",
      " [ 1 -1  1]]\n",
      "\n",
      "ELU:\n",
      " [[-0.00632121  0.          1.        ]\n",
      " [ 2.         -0.00864665  3.        ]]\n",
      "\n",
      "SELU:\n",
      " [[-1.11132754  0.          1.0507    ]\n",
      " [ 2.1014     -1.52016209  3.1521    ]]\n"
     ]
    }
   ],
   "source": [
    "# Activation function for forward propagation\n",
    "def activation_function(z, function_name=\"sigmoid\", alpha=0.01):\n",
    "    # Range: [0, 1] - used for binary classification\n",
    "    if function_name == \"sigmoid\":\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Range: [-1, 1] - zero-centred, used in hidden layers\n",
    "    elif function_name == \"tanh\":\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    # Range: [0, inf) - outputs 0 for z < 0 and z for z >= 0\n",
    "    elif function_name == \"relu\":\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    # Range: (-inf, inf) - outputs z for z >= 0 and alpha * z for z < 0\n",
    "    elif function_name == \"leaky_relu\":\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    # Range: [0, 1] - used in output layers for multi-class classification\n",
    "    elif function_name == \"softmax\":\n",
    "        exp_z = np.exp(z - np.max(z))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    # Range: (-inf, inf) - linear activation\n",
    "    elif function_name == \"linear\":\n",
    "        return z\n",
    "    \n",
    "    # Range: [0, 6] - smooth approximation of ReLU\n",
    "    elif function_name == \"softplus\":\n",
    "        return np.log1p(np.exp(z))\n",
    "    \n",
    "    # Range: [-1, 1] - scaled and shifted sigmoid\n",
    "    elif function_name == \"hard_sigmoid\":\n",
    "        return np.maximum(0, np.minimum(1, (z + 1) / 2))\n",
    "    \n",
    "    # Range: (-1, 1) - similar to tanh but with a sharper bend\n",
    "    elif function_name == \"hard_tanh\":\n",
    "        return np.maximum(-1, np.minimum(1, z))\n",
    "    \n",
    "    # Range: [0, inf) - similar to ReLU but smoother near 0\n",
    "    elif function_name == \"elu\":\n",
    "        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    # Range: (-inf, inf) - scaled ReLU\n",
    "    elif function_name == \"selu\":\n",
    "        lambda_ = 1.0507\n",
    "        alpha = 1.67326\n",
    "        return lambda_ * np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {function_name}\")\n",
    "\n",
    "# Test the function\n",
    "z = np.array([[-1, 0, 1], [2, -2, 3]])\n",
    "\n",
    "print(\"Examples of how activation functions affect the matrix: \\n\", z)\n",
    "print(\"\\nSigmoid:\\n\", activation_function(z, \"sigmoid\"))\n",
    "print(\"\\nTanh:\\n\", activation_function(z, \"tanh\"))\n",
    "print(\"\\nReLU:\\n\", activation_function(z, \"relu\"))\n",
    "print(\"\\nLeaky ReLU:\\n\", activation_function(z, \"leaky_relu\"))\n",
    "print(\"\\nSoftmax:\\n\", activation_function(z, \"softmax\"))\n",
    "print(\"\\nLinear:\\n\", activation_function(z, \"linear\"))\n",
    "print(\"\\nSoftplus:\\n\", activation_function(z, \"softplus\"))\n",
    "print(\"\\nHard Sigmoid:\\n\", activation_function(z, \"hard_sigmoid\"))\n",
    "print(\"\\nHard Tanh:\\n\", activation_function(z, \"hard_tanh\"))\n",
    "print(\"\\nELU:\\n\", activation_function(z, \"elu\"))\n",
    "print(\"\\nSELU:\\n\", activation_function(z, \"selu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for cost evaluation, between true labels and predicted values\n",
    "def loss_function(y_true, y_pred, loss_type=\"cross_entropy\", epsilon=1e-12):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels (one-hot encoded or actual values).\n",
    "    y_pred (numpy array): Predicted values (probabilities or actual values).\n",
    "    loss_type (str): Name of the loss function to use.\n",
    "    epsilon (float): Small value to avoid log(0).\n",
    "    \n",
    "    Returns:\n",
    "    float: Computed loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Categorical Cross-Entropy Loss (for multi-class classification)\n",
    "    if loss_type == \"cross_entropy\":\n",
    "        # Avoids log(0) by clipping values to be within [epsilon, 1 - epsilon]\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "    \n",
    "    elif loss_type == \"binary_cross_entropy\":\n",
    "        # Avoids log(0) by clipping values to be within [epsilon, 1 - epsilon]\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    # Mean Squared Error Loss (for regression or classification)\n",
    "    elif loss_type == \"mse\":\n",
    "        return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n",
    "\n",
    "    # Hinge Loss (used for binary classification with SVM-like models)\n",
    "    elif loss_type == \"hinge\":\n",
    "        return np.mean(np.maximum(0, 1 - y_true * y_pred))\n",
    "\n",
    "    # Kullback-Leibler Divergence (measures how one probability distribution diverges from another)\n",
    "    elif loss_type == \"kl_div\":\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Avoid log(0)\n",
    "        return np.sum(y_true * np.log(y_true / y_pred)) / y_true.shape[0]\n",
    "\n",
    "    # Focal Loss (addresses class imbalance by focusing more on hard examples)\n",
    "    elif loss_type == \"focal_loss\":\n",
    "        # Focusing parameter (can be adjusted)\n",
    "        gamma = 2.0\n",
    "        \n",
    "        # Avoids log(0) by clipping values to be within [epsilon, 1 - epsilon]\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y_true * (1 - y_pred) ** gamma * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "    \n",
    "    \n",
    "# Regularisation function to prevent overfitting by penalising large weights\n",
    "def regularisation_term(Theta1, Theta2, reg_type=\"l2\", lamb=0.1):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "    Theta1 (numpy array): Weight matrix for the first layer.\n",
    "    Theta2 (numpy array): Weight matrix for the second layer.\n",
    "    regularisation_type (str): Name of the regularisation method to use.\n",
    "    lamb (float): Regularisation parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: Computed regularisation term.\n",
    "    \"\"\"\n",
    "    \n",
    "    # L2 Regularisation (penalises large weights)\n",
    "    if reg_type == \"l2\":\n",
    "        reg_term = (lamb / 2) * (np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2))\n",
    "\n",
    "    # L1 Regularisation (penalises the absolute value of weights)\n",
    "    elif reg_type == \"l1\":\n",
    "        reg_term = (lamb / 2) * (np.sum(np.abs(Theta1[:, 1:])) + np.sum(np.abs(Theta2[:, 1:])))\n",
    "\n",
    "    # Elastic Net Regularisation (combination of L1 and L2)\n",
    "    elif reg_type == \"elastic_net\":\n",
    "        # Mixing parameter (0 <= alpha <= 1)\n",
    "        alpha = 0.5\n",
    "        \n",
    "        # Calculate L1 and L2 regularisation terms\n",
    "        l2_term = (1 - alpha) * np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2)\n",
    "        l1_term = alpha * (np.sum(np.abs(Theta1[:, 1:])) + np.sum(np.abs(Theta2[:, 1:])))\n",
    "        reg_term = (lamb / 2) * (l2_term + l1_term)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported regularisation type: {reg_type}\")\n",
    "    \n",
    "    return reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward propagation on the training data maxtrix and weight matrices\n",
    "def forward_propagation(X, Theta1, Theta2):\n",
    "    m = X.shape[0]\n",
    "    ones = np.ones((m, 1))                  # Ones matrix for bias unit (60,000 x 1)\n",
    "    X = np.append(ones, X, axis=1)          # Add bias unit to input layer (60,000 x 785)\n",
    "    a1 = X                                  # Input layer\n",
    "    z2 = np.dot(a1, Theta1.T)               # Compute z2 for hidden layer (60,000 x 100)\n",
    "    a2 = activation_function(z2, 'sigmoid')\n",
    "    a2 = np.append(ones, a2, axis=1)        # Add bias unit to hidden layer (60,000 x 101)\n",
    "    z3 = np.dot(a2, Theta2.T)               # Compute z3 for output layer (60,000 x 10)\n",
    "    a3 = activation_function(z3, 'sigmoid')\n",
    "    \n",
    "    return a1, z2, a2, z3, a3\n",
    "a1, z2, a2, z3, a3 = forward_propagation(X_train, initial_Theta1, initial_Theta2)\n",
    "\n",
    "# Calculate the cost function\n",
    "def cost_function(a3, y, Theta1, Theta2, lamb, loss_type=\"binary_cross_entropy\", reg_type=\"l2\"):\n",
    "    # Retrieve the number of samples and labels (60,000, 10)\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors (one-hot encoding)\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Calculate the cost (loss + regularisation)\n",
    "    loss = loss_function(y_vect, a3, loss_type=loss_type)\n",
    "    reg = regularisation_term(Theta1, Theta2, reg_type=reg_type, lamb=lamb)\n",
    "    J = (loss + reg) / m\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Perform back propagation on the data matrix and weight matrices\n",
    "def back_propagation(a1, z2, a2, a3, y, Theta1, Theta2):\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Compute deltas\n",
    "    Delta3 = a3 - y_vect\n",
    "    Delta2 = np.dot(Delta3, Theta2) * a2 * (1 - a2)\n",
    "    Delta2 = Delta2[:, 1:]  # Remove bias term from Delta2\n",
    "    \n",
    "    return Delta3, Delta2\n",
    "\n",
    "# Compute the gradients of the cost function\n",
    "def compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb):\n",
    "    m = a1.shape[0]\n",
    "    \n",
    "    # Set the first column of Theta1 and Theta2 to zero (bias terms are not regularized)\n",
    "    Theta1[:, 0] = 0\n",
    "    Theta2[:, 0] = 0\n",
    "    \n",
    "    # Compute the gradients\n",
    "    Theta1_grad = (1 / m) * np.dot(Delta2.T, a1) + (lamb / m) * Theta1\n",
    "    Theta2_grad = (1 / m) * np.dot(Delta3.T, a2) + (lamb / m) * Theta2\n",
    "    \n",
    "    # Flatten gradients to return a single vector\n",
    "    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "def neural_network(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    # Unroll the parameters\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))\n",
    "    \n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, Theta1, Theta2)\n",
    "    \n",
    "    # Cost function\n",
    "    J = cost_function(a3, y, Theta1, Theta2, lamb)\n",
    "    \n",
    "    # Back propagation\n",
    "    Delta3, Delta2 = back_propagation(a1, z2, a2, a3, y, Theta1, Theta2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad = compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb)\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.minimize`\n",
    "Generic optimisation function to find the minimum of a scalar function\n",
    "-   Objective Function: cost function of NN\n",
    "-   Initial Guess: initial estimate of params (weights) to optimise\n",
    "-   Method: optimisation algorithm to be used\n",
    "-   Jac: Jacobian (gradient) of objective function\n",
    "-   Options: e.g. number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        79510     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.46951D+00    |proj g|=  6.14104D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  3.55738D+00    |proj g|=  9.94175D-02\n",
      "\n",
      "At iterate    2    f=  3.28580D+00    |proj g|=  3.94858D-02\n",
      "\n",
      "At iterate    3    f=  3.21435D+00    |proj g|=  2.01788D-02\n",
      "\n",
      "At iterate    4    f=  3.18182D+00    |proj g|=  2.55385D-02\n",
      "\n",
      "At iterate    5    f=  3.09174D+00    |proj g|=  3.54507D-02\n",
      "\n",
      "At iterate    6    f=  2.86225D+00    |proj g|=  5.32777D-02\n",
      "\n",
      "At iterate    7    f=  2.23977D+00    |proj g|=  7.50375D-02\n",
      "\n",
      "At iterate    8    f=  1.80695D+00    |proj g|=  1.00171D-01\n",
      "\n",
      "At iterate    9    f=  1.45245D+00    |proj g|=  2.60632D-02\n",
      "\n",
      "At iterate   10    f=  1.41471D+00    |proj g|=  1.66493D-02\n",
      "\n",
      "At iterate   11    f=  1.32705D+00    |proj g|=  2.30244D-02\n",
      "\n",
      "At iterate   12    f=  1.18573D+00    |proj g|=  2.69241D-02\n",
      "\n",
      "At iterate   13    f=  1.09878D+00    |proj g|=  8.81562D-02\n",
      "\n",
      "At iterate   14    f=  9.80565D-01    |proj g|=  1.75419D-02\n",
      "\n",
      "At iterate   15    f=  9.48903D-01    |proj g|=  1.30911D-02\n",
      "\n",
      "At iterate   16    f=  8.88894D-01    |proj g|=  1.80235D-02\n",
      "\n",
      "At iterate   17    f=  8.29362D-01    |proj g|=  2.28303D-02\n",
      "\n",
      "At iterate   18    f=  7.75641D-01    |proj g|=  2.09317D-02\n",
      "\n",
      "At iterate   19    f=  7.45864D-01    |proj g|=  8.00757D-03\n",
      "\n",
      "At iterate   20    f=  7.18279D-01    |proj g|=  1.04943D-02\n",
      "\n",
      "At iterate   21    f=  6.97149D-01    |proj g|=  1.14157D-02\n",
      "\n",
      "At iterate   22    f=  6.53066D-01    |proj g|=  1.51899D-02\n",
      "\n",
      "At iterate   23    f=  6.24596D-01    |proj g|=  1.29845D-02\n",
      "\n",
      "At iterate   24    f=  5.98541D-01    |proj g|=  7.26725D-03\n",
      "\n",
      "At iterate   25    f=  5.77399D-01    |proj g|=  4.98041D-03\n",
      "\n",
      "At iterate   26    f=  5.59523D-01    |proj g|=  7.00263D-03\n",
      "\n",
      "At iterate   27    f=  5.32302D-01    |proj g|=  1.31344D-02\n",
      "\n",
      "At iterate   28    f=  5.04878D-01    |proj g|=  4.12388D-03\n",
      "\n",
      "At iterate   29    f=  4.90499D-01    |proj g|=  3.56708D-03\n",
      "\n",
      "At iterate   30    f=  4.70044D-01    |proj g|=  6.81553D-03\n",
      "\n",
      "At iterate   31    f=  4.60770D-01    |proj g|=  1.68255D-02\n",
      "\n",
      "At iterate   32    f=  4.43586D-01    |proj g|=  5.57276D-03\n",
      "\n",
      "At iterate   33    f=  4.32016D-01    |proj g|=  3.22114D-03\n",
      "\n",
      "At iterate   34    f=  4.18773D-01    |proj g|=  5.48105D-03\n",
      "\n",
      "At iterate   35    f=  4.01156D-01    |proj g|=  5.60403D-03\n",
      "\n",
      "At iterate   36    f=  3.96331D-01    |proj g|=  1.22990D-02\n",
      "\n",
      "At iterate   37    f=  3.74470D-01    |proj g|=  2.93986D-03\n",
      "\n",
      "At iterate   38    f=  3.66934D-01    |proj g|=  2.56949D-03\n",
      "\n",
      "At iterate   39    f=  3.55141D-01    |proj g|=  3.59973D-03\n",
      "\n",
      "At iterate   40    f=  3.42290D-01    |proj g|=  3.87196D-03\n",
      "\n",
      "At iterate   41    f=  3.32833D-01    |proj g|=  6.53553D-03\n",
      "\n",
      "At iterate   42    f=  3.21435D-01    |proj g|=  1.78285D-03\n",
      "\n",
      "At iterate   43    f=  3.14651D-01    |proj g|=  2.13935D-03\n",
      "\n",
      "At iterate   44    f=  3.05698D-01    |proj g|=  3.23675D-03\n",
      "\n",
      "At iterate   45    f=  2.98239D-01    |proj g|=  8.38782D-03\n",
      "\n",
      "At iterate   46    f=  2.87632D-01    |proj g|=  2.34160D-03\n",
      "\n",
      "At iterate   47    f=  2.80486D-01    |proj g|=  1.89687D-03\n",
      "\n",
      "At iterate   48    f=  2.73850D-01    |proj g|=  2.59286D-03\n",
      "\n",
      "At iterate   49    f=  2.63390D-01    |proj g|=  3.63381D-03\n",
      "\n",
      "At iterate   50    f=  2.58701D-01    |proj g|=  6.73492D-03\n",
      "\n",
      "At iterate   51    f=  2.48450D-01    |proj g|=  1.38338D-03\n",
      "\n",
      "At iterate   52    f=  2.45212D-01    |proj g|=  1.42817D-03\n",
      "\n",
      "At iterate   53    f=  2.39842D-01    |proj g|=  2.17455D-03\n",
      "\n",
      "At iterate   54    f=  2.35481D-01    |proj g|=  5.70693D-03\n",
      "\n",
      "At iterate   55    f=  2.28535D-01    |proj g|=  1.66695D-03\n",
      "\n",
      "At iterate   56    f=  2.22080D-01    |proj g|=  1.86456D-03\n",
      "\n",
      "At iterate   57    f=  2.17603D-01    |proj g|=  1.79756D-03\n",
      "\n",
      "At iterate   58    f=  2.14965D-01    |proj g|=  7.93293D-03\n",
      "\n",
      "At iterate   59    f=  2.07943D-01    |proj g|=  2.03575D-03\n",
      "\n",
      "At iterate   60    f=  2.04206D-01    |proj g|=  1.27438D-03\n",
      "\n",
      "At iterate   61    f=  2.00337D-01    |proj g|=  1.90475D-03\n",
      "\n",
      "At iterate   62    f=  1.94845D-01    |proj g|=  2.11027D-03\n",
      "\n",
      "At iterate   63    f=  1.91903D-01    |proj g|=  6.02022D-03\n",
      "\n",
      "At iterate   64    f=  1.84600D-01    |proj g|=  1.27951D-03\n",
      "\n",
      "At iterate   65    f=  1.82058D-01    |proj g|=  1.03157D-03\n",
      "\n",
      "At iterate   66    f=  1.77939D-01    |proj g|=  1.71505D-03\n",
      "\n",
      "At iterate   67    f=  1.74235D-01    |proj g|=  5.13492D-03\n",
      "\n",
      "At iterate   68    f=  1.68619D-01    |proj g|=  1.55169D-03\n",
      "\n",
      "At iterate   69    f=  1.65346D-01    |proj g|=  9.45740D-04\n",
      "\n",
      "At iterate   70    f=  1.62118D-01    |proj g|=  1.37751D-03\n",
      "\n",
      "At iterate   71    f=  1.57486D-01    |proj g|=  3.15432D-03\n",
      "\n",
      "At iterate   72    f=  1.52761D-01    |proj g|=  1.20837D-03\n",
      "\n",
      "At iterate   73    f=  1.49468D-01    |proj g|=  9.44557D-04\n"
     ]
    }
   ],
   "source": [
    "# Package all arguments into a tuple for optimisation function\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n",
    "\n",
    "# Minimise the cost function using the L-BFGS-B optimisation algorithm\n",
    "results = minimize(neural_network, x0=initial_nn_params, args=myargs, options={'disp': True, 'maxiter': maxiter}, method='L-BFGS-B', jac=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Objective / Cost function`\n",
    "| Variable | Definition                                                                     |\n",
    "| -------- | -------                                                                        |\n",
    "| N        | total number of variables (parameters, weights)                                |\n",
    "| M        | number of corrections (updates) stored in memory by the algorithm              |\n",
    "| X0       | none of the variables are at their bounds                                      |\n",
    "| f        | current value of cost function                                                 |\n",
    "| proj g   | norm of projected gradient (how close optimisation is to a stationary point)   |\n",
    "| success  | whether optimisation converged or not (did not)                                |\n",
    "| fun      | value of cost function @ final params                                          |\n",
    "| x        | final values of params (weights)                                               |\n",
    "| nit      | number of iterations performed                                                 |\n",
    "| jac      | gradient of cost function @ final params                                       |\n",
    "| nfev     | # times cost function was evaluated                                            |\n",
    "| njev     | # times gradient was evaluated                                                 |\n",
    "| hess_inv | inverse of the approx Hessian matrix @ final point (helps understand curvature)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised weights: \n",
      " [-0.35802313 -0.07246305 -0.09038717 ...  1.03113945 -3.53644999\n",
      "  2.61600917] \n",
      " (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Extract the optimised weights (trained Theta)\n",
    "nn_params = results[\"x\"]\n",
    "print(\"Optimised weights: \\n\", nn_params, \"\\n\", nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 785) (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Split weights back into Theta1 (100 x 785) and Theta2 (10 x 101)\n",
    "Theta1 = np.reshape(nn_params[:(hidden_layer_size * (input_layer_size + 1))], (hidden_layer_size, (input_layer_size + 1)))\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):], (num_labels, (hidden_layer_size + 1)))\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  97.44\n",
      "Train set accuracy:  99.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Performs forward propagation to predict the label (digit) of an input image\n",
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0] \n",
    "    ones = np.ones((m, 1))                      # Ones matrix\n",
    "    X = np.append(ones, X, axis=1)              # Add bias unit to input (first) layer \n",
    "    z2 = np.dot(X, Theta1.transpose()) \n",
    "    a2 = 1 / (1 + np.exp(-z2))                  # Activation for hidden (second) layer\n",
    "    ones = np.ones((m, 1)) \n",
    "    a2 = np.append(ones, a2, axis=1)            # Adding bias unit to hidden layer \n",
    "    z3 = np.dot(a2, Theta2.transpose()) \n",
    "    a3 = 1 / (1 + np.exp(-z3))                  # Activation for output (third) layer \n",
    "    p = (np.argmax(a3, axis=1))                 # Predicting the class on the basis of max value of hypothesis \n",
    "    return p \n",
    "\n",
    "\n",
    "# Check test set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_test)\n",
    "print(\"Test set accuracy: \", np.mean(pred == y_test) * 100)\n",
    "\n",
    "# Check train set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_train)\n",
    "print(\"Train set accuracy: \", np.mean(pred == y_train) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate precision of model \n",
    "true_positive = 0\n",
    "\n",
    "# Iterate through predictions and compare with actual labels\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == y_train[i]:\n",
    "        true_positive += 1\n",
    "        \n",
    "# Calculate precision\n",
    "false_positive = len(y_train) - true_positive \n",
    "print('Precision =', true_positive/(true_positive + false_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Thetas in .txt files: (100 x 785) and (10 x 101)\n",
    "np.savetxt('Theta1.txt', Theta1, delimiter=' ') \n",
    "np.savetxt('Theta2.txt', Theta2, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop up widget to draw digit and output model prediction\n",
    "window = Tk() \n",
    "window.title(\"Handwritten digit recognition\") \n",
    "l1 = Label() \n",
    "\n",
    "\n",
    "def MyProject():\n",
    "\tglobal l1 \n",
    "\n",
    "\twidget = cv \n",
    "\t# Setting co-ordinates of canvas\n",
    "\tx = window.winfo_rootx() + widget.winfo_x() \n",
    "\ty = window.winfo_rooty() + widget.winfo_y() \n",
    "\tx1 = x + widget.winfo_width() \n",
    "\ty1 = y + widget.winfo_height() \n",
    "\n",
    "\t# Image is captured from canvas and is resized to (28 X 28) px \n",
    "\timg = ImageGrab.grab().crop((x, y, x1, y1)).resize((28, 28)) \n",
    "\n",
    "\t# Converting rgb to grayscale image \n",
    "\timg = img.convert('L') \n",
    "\n",
    "\t# Extracting pixel matrix of image and converting it to a vector of (1, 784) \n",
    "\tx = np.asarray(img) \n",
    "\tvec = np.zeros((1, 784)) \n",
    "\tk = 0\n",
    "\tfor i in range(28): \n",
    "\t\tfor j in range(28): \n",
    "\t\t\tvec[0][k] = x[i][j] \n",
    "\t\t\tk += 1\n",
    "\n",
    "\t# Loading Thetas \n",
    "\tTheta1 = np.loadtxt('Theta1.txt') \n",
    "\tTheta2 = np.loadtxt('Theta2.txt')\n",
    "\n",
    "\t# Calling function for prediction \n",
    "\tpred = predict(Theta1, Theta2, vec / 255)\n",
    "\n",
    "\t# Displaying the result \n",
    "\tl1 = Label(window, text=\"Digit = \" + str(pred[0]), font=('Algerian', 20)) \n",
    "\tl1.place(x=230, y=420) \n",
    "\n",
    "\n",
    "lastx, lasty = None, None\n",
    "\n",
    "\n",
    "# Clears the canvas \n",
    "def clear_widget(): \n",
    "\tglobal cv, l1 \n",
    "\tcv.delete(\"all\") \n",
    "\tl1.destroy() \n",
    "\n",
    "\n",
    "# Activate canvas\n",
    "def event_activation(event): \n",
    "\tglobal lastx, lasty \n",
    "\tcv.bind('<B1-Motion>', draw_lines) \n",
    "\tlastx, lasty = event.x, event.y \n",
    "\n",
    "\n",
    "# To draw on canvas \n",
    "def draw_lines(event): \n",
    "\tglobal lastx, lasty \n",
    "\tx, y = event.x, event.y \n",
    "\tcv.create_line((lastx, lasty, x, y), width=30, fill='white', capstyle=ROUND, smooth=TRUE, splinesteps=12) \n",
    "\tlastx, lasty = x, y \n",
    "\n",
    "\n",
    "# Label \n",
    "L1 = Label(window, text=\"Handwritten Digit Recoginition\", font=('Algerian', 25), fg=\"blue\") \n",
    "L1.place(x=35, y=10) \n",
    "\n",
    "# Button to clear canvas \n",
    "b1 = Button(window, text=\"1. Clear Canvas\", font=('Algerian', 15), bg=\"orange\", fg=\"black\", command=clear_widget) \n",
    "b1.place(x=120, y=370) \n",
    "\n",
    "# Button to predict digit drawn on canvas \n",
    "b2 = Button(window, text=\"2. Prediction\", font=('Algerian', 15), bg=\"white\", fg=\"red\", command=MyProject) \n",
    "b2.place(x=320, y=370) \n",
    "\n",
    "# Setting properties of canvas \n",
    "cv = Canvas(window, width=350, height=290, bg='black') \n",
    "cv.place(x=120, y=70) \n",
    "\n",
    "cv.bind('<Button-1>', event_activation) \n",
    "window.geometry(\"600x500\") \n",
    "window.mainloop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
