{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST dataset of digit recognition\n",
    "\n",
    "Tutorial: https://www.geeksforgeeks.org/handwritten-digit-recognition-using-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tkinter import *\n",
    "from PIL import ImageGrab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "normalisation_factor = 1/255        # Each pixel = 8-bit integer (0-255)\n",
    "input_layer_size = 28 * 28          # Number of features (pixels)\n",
    "hidden_layer_size = 100             # Number of hidden units\n",
    "num_labels = 10                     # Number of labels (0-9)\n",
    "maxiter = 100                       # Maximum number of iterations for optimization\n",
    "lambda_reg = 0.1                    # Regularization parameter (prevents overfitting)\n",
    "epsilon = 0.15                      # Random initialisation parameter (prevents symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  dict_keys(['__header__', '__version__', '__globals__', 'mldata_descr_ordering', 'data', 'label'])\n",
      "Dataset shape:  (784, 70000)\n"
     ]
    }
   ],
   "source": [
    "# Load mat file of data\n",
    "data = loadmat('../data/mnist-original.mat')\n",
    "print(\"Keys: \", data.keys())\n",
    "print(\"Dataset shape: \", data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data, X:\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      " (70000, 784)\n",
      "\n",
      "Labels, y:\n",
      "  [0. 0. 0. ... 9. 9. 9.] \n",
      " (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and transpose\n",
    "X = data['data'].T\n",
    "\n",
    "# Normalise the data so that each pixel is in the range [0, 1]\n",
    "X = X * normalisation_factor\n",
    "\n",
    "# Extract labels from data and flatten\n",
    "y = data['label'].flatten()\n",
    "\n",
    "print(\"Data, X:\\n \", X, \"\\n\", X.shape)\n",
    "print(\"\\nLabels, y:\\n \", y, \"\\n\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  (60000, 784) (60000,)\n",
      "Testing size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set with 60,000 samples and test set with 10,000 samples (capital for matrix, lower case for vector)\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000]\n",
    "print(\"Training size: \", X_train.shape, y_train.shape)\n",
    "\n",
    "# (2nd colon specifies all columns)\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:]\n",
    "print(\"Testing size: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample image (random by default)\n",
    "def show_image(X, y, index=None):\n",
    "    # Choose a random index if none is provided\n",
    "    if index is None:\n",
    "        N = X.shape[0]\n",
    "        index = np.random.randint(0, N - 1)\n",
    "    \n",
    "    # Extract image data and label\n",
    "    image = X[index].reshape(28, 28)\n",
    "    label = y[index]\n",
    "    \n",
    "    # Display image and label\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Choose a random index from the total number of images\n",
    "show_image(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1:\n",
      "  [[-0.04527519  0.08645128  0.14967121 ...  0.07467268  0.06095429\n",
      "   0.04433275]\n",
      " [-0.09904031  0.0290568  -0.14613074 ... -0.10664474 -0.02712011\n",
      "  -0.02680306]\n",
      " [ 0.01665128  0.0030571  -0.05333852 ...  0.05423247 -0.12142121\n",
      "  -0.08264388]\n",
      " ...\n",
      " [ 0.137579    0.01700141 -0.03070998 ... -0.08180259 -0.11630904\n",
      "   0.10520473]\n",
      " [ 0.14407801 -0.10849045  0.00265089 ... -0.09612479 -0.14631937\n",
      "   0.1313735 ]\n",
      " [ 0.13763594 -0.13086071  0.00963977 ...  0.13377163  0.14006473\n",
      "  -0.05710927]] \n",
      " (100, 785)\n",
      "\n",
      "Theta2:\n",
      "  [[-0.00038299  0.00455358  0.01991931 ... -0.06763254 -0.13778855\n",
      "   0.03102605]\n",
      " [ 0.01053874 -0.14669422  0.01132565 ...  0.1010875   0.06238535\n",
      "  -0.02759254]\n",
      " [-0.0211659   0.07241686  0.03259435 ... -0.08010295  0.01874264\n",
      "  -0.09616889]\n",
      " ...\n",
      " [-0.13047035  0.00480696  0.10801566 ... -0.14478112  0.10217228\n",
      "  -0.01086469]\n",
      " [-0.14162883 -0.09464915 -0.09268031 ... -0.12021677 -0.0103491\n",
      "   0.12441632]\n",
      " [ 0.06603339  0.06658141 -0.12491322 ...  0.14075797 -0.04980631\n",
      "   0.11596443]] \n",
      " (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly initialise Thetas (weights) between a range of [-epsilon, epsilon]\n",
    "def initialise(a, b, epsilon):\n",
    "    # Scale and shift random values to be within range\n",
    "    c = (np.random.rand(a, b + 1) * (2 * epsilon)) - epsilon\n",
    "    \n",
    "    # Returns matrix of randomly initialised weights, of dimensions a x (b + 1)\n",
    "    return c\n",
    "\n",
    "# epsilon chosen arbitrarily (small enough to avoid saturation and large enough to avoid vanishing gradients)\n",
    "initial_Theta1 = initialise(hidden_layer_size, input_layer_size, epsilon)\n",
    "initial_Theta2 = initialise(num_labels, hidden_layer_size, epsilon)\n",
    "print(\"Theta1:\\n \", initial_Theta1, \"\\n\", initial_Theta1.shape)\n",
    "print(\"\\nTheta2:\\n \", initial_Theta2, \"\\n\", initial_Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters:  (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Unroll (combine) the weight matrices into a single column vector (easier for optimisation algorithm)\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "print(\"Initial parameters: \", initial_nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why 79,510?\n",
    "`Theta1`\n",
    "-   hidden_layer_size * (input_layer_size + 1)\n",
    "-   100 * (784 + 1)\n",
    "-   78,500\n",
    "\n",
    "`Theta2`\n",
    "-   num_labels * (hidden_layer_size + 1)\n",
    "-   10 * (100 + 1)\n",
    "-   1,010\n",
    "\n",
    "`TOTAL`\n",
    "-   79,510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of how activation functions affect the matrix: \n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Sigmoid:\n",
      " [[0.26894142 0.5        0.73105858]\n",
      " [0.88079708 0.11920292 0.95257413]]\n",
      "\n",
      "Tanh:\n",
      " [[-0.76159416  0.          0.76159416]\n",
      " [ 0.96402758 -0.96402758  0.99505475]]\n",
      "\n",
      "ReLU:\n",
      " [[0 0 1]\n",
      " [2 0 3]]\n",
      "\n",
      "Leaky ReLU:\n",
      " [[-0.01  0.    1.  ]\n",
      " [ 2.   -0.02  3.  ]]\n",
      "\n",
      "Softmax:\n",
      " [[0.09003057 0.24472847 0.66524096]\n",
      " [0.26762315 0.00490169 0.72747516]]\n",
      "\n",
      "Linear:\n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Softplus:\n",
      " [[0.31326169 0.69314718 1.31326169]\n",
      " [2.12692801 0.12692801 3.04858735]]\n",
      "\n",
      "Hard Sigmoid:\n",
      " [[0.  0.5 1. ]\n",
      " [1.  0.  1. ]]\n",
      "\n",
      "Hard Tanh:\n",
      " [[-1  0  1]\n",
      " [ 1 -1  1]]\n",
      "\n",
      "ELU:\n",
      " [[-0.00632121  0.          1.        ]\n",
      " [ 2.         -0.00864665  3.        ]]\n",
      "\n",
      "SELU:\n",
      " [[-1.11132754  0.          1.0507    ]\n",
      " [ 2.1014     -1.52016209  3.1521    ]]\n"
     ]
    }
   ],
   "source": [
    "# Activation function for forward propagation\n",
    "def activation_function(z, function_name=\"sigmoid\", alpha=0.01):\n",
    "    # Range: [0, 1] - used for binary classification\n",
    "    if function_name == \"sigmoid\":\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Range: [-1, 1] - zero-centred, used in hidden layers\n",
    "    elif function_name == \"tanh\":\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    # Range: [0, inf) - outputs 0 for z < 0 and z for z >= 0\n",
    "    elif function_name == \"relu\":\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    # Range: (-inf, inf) - outputs z for z >= 0 and alpha * z for z < 0\n",
    "    elif function_name == \"leaky_relu\":\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    # Range: [0, 1] - used in output layers for multi-class classification\n",
    "    elif function_name == \"softmax\":\n",
    "        exp_z = np.exp(z - np.max(z))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    # Range: (-inf, inf) - linear activation\n",
    "    elif function_name == \"linear\":\n",
    "        return z\n",
    "    \n",
    "    # Range: [0, 6] - smooth approximation of ReLU\n",
    "    elif function_name == \"softplus\":\n",
    "        return np.log1p(np.exp(z))\n",
    "    \n",
    "    # Range: [-1, 1] - scaled and shifted sigmoid\n",
    "    elif function_name == \"hard_sigmoid\":\n",
    "        return np.maximum(0, np.minimum(1, (z + 1) / 2))\n",
    "    \n",
    "    # Range: (-1, 1) - similar to tanh but with a sharper bend\n",
    "    elif function_name == \"hard_tanh\":\n",
    "        return np.maximum(-1, np.minimum(1, z))\n",
    "    \n",
    "    # Range: [0, inf) - similar to ReLU but smoother near 0\n",
    "    elif function_name == \"elu\":\n",
    "        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    # Range: (-inf, inf) - scaled ReLU\n",
    "    elif function_name == \"selu\":\n",
    "        lambda_ = 1.0507\n",
    "        alpha = 1.67326\n",
    "        return lambda_ * np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {function_name}\")\n",
    "\n",
    "# Test the function\n",
    "z = np.array([[-1, 0, 1], [2, -2, 3]])\n",
    "\n",
    "print(\"Examples of how activation functions affect the matrix: \\n\", z)\n",
    "print(\"\\nSigmoid:\\n\", activation_function(z, \"sigmoid\"))\n",
    "print(\"\\nTanh:\\n\", activation_function(z, \"tanh\"))\n",
    "print(\"\\nReLU:\\n\", activation_function(z, \"relu\"))\n",
    "print(\"\\nLeaky ReLU:\\n\", activation_function(z, \"leaky_relu\"))\n",
    "print(\"\\nSoftmax:\\n\", activation_function(z, \"softmax\"))\n",
    "print(\"\\nLinear:\\n\", activation_function(z, \"linear\"))\n",
    "print(\"\\nSoftplus:\\n\", activation_function(z, \"softplus\"))\n",
    "print(\"\\nHard Sigmoid:\\n\", activation_function(z, \"hard_sigmoid\"))\n",
    "print(\"\\nHard Tanh:\\n\", activation_function(z, \"hard_tanh\"))\n",
    "print(\"\\nELU:\\n\", activation_function(z, \"elu\"))\n",
    "print(\"\\nSELU:\\n\", activation_function(z, \"selu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward propagation on the training data maxtrix and weight matrices\n",
    "def forward_propagation(X, Theta1, Theta2):\n",
    "    m = X.shape[0]\n",
    "    ones = np.ones((m, 1))                  # Ones matrix for bias unit (60,000 x 1)\n",
    "    X = np.append(ones, X, axis=1)          # Add bias unit to input layer (60,000 x 785)\n",
    "    a1 = X                                  # Input layer\n",
    "    z2 = np.dot(a1, Theta1.T)               # Compute z2 for hidden layer (60,000 x 100)\n",
    "    a2 = activation_function(z2, 'sigmoid')\n",
    "    a2 = np.append(ones, a2, axis=1)        # Add bias unit to hidden layer (60,000 x 101)\n",
    "    z3 = np.dot(a2, Theta2.T)               # Compute z3 for output layer (60,000 x 10)\n",
    "    a3 = activation_function(z3, 'sigmoid')\n",
    "    \n",
    "    return a1, z2, a2, z3, a3\n",
    "\n",
    "# Calculate the cost function\n",
    "def cost_function(a3, y, Theta1, Theta2, lamb):\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Calculate the cost (loss + regularization)\n",
    "    J = (\n",
    "        (1 / m) * (np.sum(-y_vect * np.log(a3) - (1 - y_vect) * np.log(1 - a3)))\n",
    "        + \n",
    "        (lamb / (2 * m)) * (np.sum(np.sum(Theta1[:, 1:] ** 2)) + np.sum(np.sum(Theta2[:, 1:] ** 2)))\n",
    "    )\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Perform back propagation on the data matrix and weight matrices\n",
    "def back_propagation(a1, z2, a2, a3, y, Theta1, Theta2):\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Compute deltas\n",
    "    Delta3 = a3 - y_vect\n",
    "    Delta2 = np.dot(Delta3, Theta2) * a2 * (1 - a2)\n",
    "    Delta2 = Delta2[:, 1:]  # Remove bias term from Delta2\n",
    "    \n",
    "    return Delta3, Delta2\n",
    "\n",
    "# Compute the gradients of the cost function\n",
    "def compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb):\n",
    "    m = a1.shape[0]\n",
    "    \n",
    "    # Set the first column of Theta1 and Theta2 to zero (bias terms are not regularized)\n",
    "    Theta1[:, 0] = 0\n",
    "    Theta2[:, 0] = 0\n",
    "    \n",
    "    # Compute the gradients\n",
    "    Theta1_grad = (1 / m) * np.dot(Delta2.T, a1) + (lamb / m) * Theta1\n",
    "    Theta2_grad = (1 / m) * np.dot(Delta3.T, a2) + (lamb / m) * Theta2\n",
    "    \n",
    "    # Flatten gradients to return a single vector\n",
    "    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "def neural_network(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    # Unroll the parameters\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))\n",
    "    \n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, Theta1, Theta2)\n",
    "    \n",
    "    # Cost function\n",
    "    J = cost_function(a3, y, Theta1, Theta2, lamb)\n",
    "    \n",
    "    # Back propagation\n",
    "    Delta3, Delta2 = back_propagation(a1, z2, a2, a3, y, Theta1, Theta2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad = compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb)\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.minimize`\n",
    "Generic optimisation function to find the minimum of a scalar function\n",
    "-   Objective Function: cost function of NN\n",
    "-   Initial Guess: initial estimate of params (weights) to optimise\n",
    "-   Method: optimisation algorithm to be used\n",
    "-   Jac: Jacobian (gradient) of objective function\n",
    "-   Options: e.g. number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        79510     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.67378D+00    |proj g|=  5.15862D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  3.35710D+00    |proj g|=  5.89480D-02\n",
      "\n",
      "At iterate    2    f=  3.25141D+00    |proj g|=  3.61778D-02\n",
      "\n",
      "At iterate    3    f=  3.20114D+00    |proj g|=  2.49376D-02\n",
      "\n",
      "At iterate    4    f=  3.15153D+00    |proj g|=  3.00432D-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m myargs \u001b[38;5;241m=\u001b[39m (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Minimise the cost function using the L-BFGS-B optimisation algorithm\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_nn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:343\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[33], line 87\u001b[0m, in \u001b[0;36mneural_network\u001b[0;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb)\u001b[0m\n\u001b[1;32m     84\u001b[0m Delta3, Delta2 \u001b[38;5;241m=\u001b[39m back_propagation(a1, z2, a2, a3, y, Theta1, Theta2)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDelta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDelta3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m J, grad\n",
      "Cell \u001b[0;32mIn[33], line 62\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb)\u001b[0m\n\u001b[1;32m     59\u001b[0m Theta2[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Compute the gradients\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m Theta1_grad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDelta2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (lamb \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m Theta1\n\u001b[1;32m     63\u001b[0m Theta2_grad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Delta3\u001b[38;5;241m.\u001b[39mT, a2) \u001b[38;5;241m+\u001b[39m (lamb \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m Theta2\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Flatten gradients to return a single vector\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Package all arguments into a tuple for optimisation function\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n",
    "\n",
    "# Minimise the cost function using the L-BFGS-B optimisation algorithm\n",
    "results = minimize(neural_network, x0=initial_nn_params, args=myargs, options={'disp': True, 'maxiter': maxiter}, method='L-BFGS-B', jac=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Objective / Cost function`\n",
    "| Variable | Definition                                                                     |\n",
    "| -------- | -------                                                                        |\n",
    "| N        | total number of variables (parameters, weights)                                |\n",
    "| M        | number of corrections (updates) stored in memory by the algorithm              |\n",
    "| X0       | none of the variables are at their bounds                                      |\n",
    "| f        | current value of cost function                                                 |\n",
    "| proj g   | norm of projected gradient (how close optimisation is to a stationary point)   |\n",
    "| success  | whether optimisation converged or not (did not)                                |\n",
    "| fun      | value of cost function @ final params                                          |\n",
    "| x        | final values of params (weights)                                               |\n",
    "| nit      | number of iterations performed                                                 |\n",
    "| jac      | gradient of cost function @ final params                                       |\n",
    "| nfev     | # times cost function was evaluated                                            |\n",
    "| njev     | # times gradient was evaluated                                                 |\n",
    "| hess_inv | inverse of the approx Hessian matrix @ final point (helps understand curvature)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised weights: \n",
      " [-0.58609539  0.13218445  0.11142981 ... -2.93568529  0.56368882\n",
      "  0.02294904] \n",
      " (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Extract the optimised weights (trained Theta)\n",
    "nn_params = results[\"x\"]\n",
    "print(\"Optimised weights: \\n\", nn_params, \"\\n\", nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 785) (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Split weights back into Theta1 (100 x 785) and Theta2 (10 x 101)\n",
    "Theta1 = np.reshape(nn_params[:(hidden_layer_size * (input_layer_size + 1))], (hidden_layer_size, (input_layer_size + 1)))\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):], (num_labels, (hidden_layer_size + 1)))\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  97.28\n",
      "Train set accuracy:  99.50333333333333\n"
     ]
    }
   ],
   "source": [
    "# Performs forward propagation to predict the label (digit) of an input image\n",
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0] \n",
    "    ones = np.ones((m, 1))                      # Ones matrix\n",
    "    X = np.append(ones, X, axis=1)              # Add bias unit to input (first) layer \n",
    "    z2 = np.dot(X, Theta1.transpose()) \n",
    "    a2 = 1 / (1 + np.exp(-z2))                  # Activation for hidden (second) layer\n",
    "    ones = np.ones((m, 1)) \n",
    "    a2 = np.append(ones, a2, axis=1)            # Adding bias unit to hidden layer \n",
    "    z3 = np.dot(a2, Theta2.transpose()) \n",
    "    a3 = 1 / (1 + np.exp(-z3))                  # Activation for output (third) layer \n",
    "    p = (np.argmax(a3, axis=1))                 # Predicting the class on the basis of max value of hypothesis \n",
    "    return p \n",
    "\n",
    "\n",
    "# Check test set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_test)\n",
    "print(\"Test set accuracy: \", np.mean(pred == y_test) * 100)\n",
    "\n",
    "# Check train set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_train)\n",
    "print(\"Train set accuracy: \", np.mean(pred == y_train) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9950333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate precision of model \n",
    "true_positive = 0\n",
    "\n",
    "# Iterate through predictions and compare with actual labels\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == y_train[i]:\n",
    "        true_positive += 1\n",
    "        \n",
    "# Calculate precision\n",
    "false_positive = len(y_train) - true_positive \n",
    "print('Precision =', true_positive/(true_positive + false_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Thetas in .txt files: (100 x 785) and (10 x 101)\n",
    "np.savetxt('Theta1.txt', Theta1, delimiter=' ') \n",
    "np.savetxt('Theta2.txt', Theta2, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop up widget to draw digit and output model prediction\n",
    "window = Tk() \n",
    "window.title(\"Handwritten digit recognition\") \n",
    "l1 = Label() \n",
    "\n",
    "\n",
    "def MyProject():\n",
    "\tglobal l1 \n",
    "\n",
    "\twidget = cv \n",
    "\t# Setting co-ordinates of canvas\n",
    "\tx = window.winfo_rootx() + widget.winfo_x() \n",
    "\ty = window.winfo_rooty() + widget.winfo_y() \n",
    "\tx1 = x + widget.winfo_width() \n",
    "\ty1 = y + widget.winfo_height() \n",
    "\n",
    "\t# Image is captured from canvas and is resized to (28 X 28) px \n",
    "\timg = ImageGrab.grab().crop((x, y, x1, y1)).resize((28, 28)) \n",
    "\n",
    "\t# Converting rgb to grayscale image \n",
    "\timg = img.convert('L') \n",
    "\n",
    "\t# Extracting pixel matrix of image and converting it to a vector of (1, 784) \n",
    "\tx = np.asarray(img) \n",
    "\tvec = np.zeros((1, 784)) \n",
    "\tk = 0\n",
    "\tfor i in range(28): \n",
    "\t\tfor j in range(28): \n",
    "\t\t\tvec[0][k] = x[i][j] \n",
    "\t\t\tk += 1\n",
    "\n",
    "\t# Loading Thetas \n",
    "\tTheta1 = np.loadtxt('Theta1.txt') \n",
    "\tTheta2 = np.loadtxt('Theta2.txt')\n",
    "\n",
    "\t# Calling function for prediction \n",
    "\tpred = predict(Theta1, Theta2, vec / 255)\n",
    "\n",
    "\t# Displaying the result \n",
    "\tl1 = Label(window, text=\"Digit = \" + str(pred[0]), font=('Algerian', 20)) \n",
    "\tl1.place(x=230, y=420) \n",
    "\n",
    "\n",
    "lastx, lasty = None, None\n",
    "\n",
    "\n",
    "# Clears the canvas \n",
    "def clear_widget(): \n",
    "\tglobal cv, l1 \n",
    "\tcv.delete(\"all\") \n",
    "\tl1.destroy() \n",
    "\n",
    "\n",
    "# Activate canvas\n",
    "def event_activation(event): \n",
    "\tglobal lastx, lasty \n",
    "\tcv.bind('<B1-Motion>', draw_lines) \n",
    "\tlastx, lasty = event.x, event.y \n",
    "\n",
    "\n",
    "# To draw on canvas \n",
    "def draw_lines(event): \n",
    "\tglobal lastx, lasty \n",
    "\tx, y = event.x, event.y \n",
    "\tcv.create_line((lastx, lasty, x, y), width=30, fill='white', capstyle=ROUND, smooth=TRUE, splinesteps=12) \n",
    "\tlastx, lasty = x, y \n",
    "\n",
    "\n",
    "# Label \n",
    "L1 = Label(window, text=\"Handwritten Digit Recoginition\", font=('Algerian', 25), fg=\"blue\") \n",
    "L1.place(x=35, y=10) \n",
    "\n",
    "# Button to clear canvas \n",
    "b1 = Button(window, text=\"1. Clear Canvas\", font=('Algerian', 15), bg=\"orange\", fg=\"black\", command=clear_widget) \n",
    "b1.place(x=120, y=370) \n",
    "\n",
    "# Button to predict digit drawn on canvas \n",
    "b2 = Button(window, text=\"2. Prediction\", font=('Algerian', 15), bg=\"white\", fg=\"red\", command=MyProject) \n",
    "b2.place(x=320, y=370) \n",
    "\n",
    "# Setting properties of canvas \n",
    "cv = Canvas(window, width=350, height=290, bg='black') \n",
    "cv.place(x=120, y=70) \n",
    "\n",
    "cv.bind('<Button-1>', event_activation) \n",
    "window.geometry(\"600x500\") \n",
    "window.mainloop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
