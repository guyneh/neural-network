{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST dataset of digit recognition\n",
    "\n",
    "Tutorial: https://www.geeksforgeeks.org/handwritten-digit-recognition-using-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tkinter import *\n",
    "from PIL import ImageGrab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "normalisation_factor = 1/255        # Each pixel = 8-bit integer (0-255)\n",
    "input_layer_size = 28 * 28          # Number of features (pixels)\n",
    "hidden_layer_size = 100             # Number of hidden units\n",
    "num_labels = 10                     # Number of labels (0-9)\n",
    "maxiter = 100                       # Maximum number of iterations for optimisation\n",
    "lambda_reg = 0.1                    # Regularisation parameter (prevents overfitting)\n",
    "epsilon = 0.15                      # Random initialisation parameter (prevents symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  dict_keys(['__header__', '__version__', '__globals__', 'mldata_descr_ordering', 'data', 'label'])\n",
      "Dataset shape:  (784, 70000)\n"
     ]
    }
   ],
   "source": [
    "# Load mat file of data\n",
    "data = loadmat('../data/mnist-original.mat')\n",
    "print(\"Keys: \", data.keys())\n",
    "print(\"Dataset shape: \", data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data, X:\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      " (70000, 784)\n",
      "\n",
      "Labels, y:\n",
      "  [0. 0. 0. ... 9. 9. 9.] \n",
      " (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and transpose\n",
    "X = data['data'].T\n",
    "\n",
    "# Normalise the data so that each pixel is in the range [0, 1]\n",
    "X = X * normalisation_factor\n",
    "\n",
    "# Extract labels from data and flatten\n",
    "y = data['label'].flatten()\n",
    "\n",
    "print(\"Data, X:\\n \", X, \"\\n\", X.shape)\n",
    "print(\"\\nLabels, y:\\n \", y, \"\\n\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  (60000, 784) (60000,)\n",
      "Testing size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set with 60,000 samples and test set with 10,000 samples (capital for matrix, lower case for vector)\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000]\n",
    "print(\"Training size: \", X_train.shape, y_train.shape)\n",
    "\n",
    "# (2nd colon specifies all columns)\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:]\n",
    "print(\"Testing size: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample image (random by default)\n",
    "def show_image(X, y, index=None):\n",
    "    # Choose a random index if none is provided\n",
    "    if index is None:\n",
    "        N = X.shape[0]\n",
    "        index = np.random.randint(0, N - 1)\n",
    "    \n",
    "    # Extract image data and label\n",
    "    image = X[index].reshape(28, 28)\n",
    "    label = y[index]\n",
    "    \n",
    "    # Display image and label\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Choose a random index from the total number of images\n",
    "show_image(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1:\n",
      "  [[-0.11498966  0.13351023 -0.0723772  ...  0.00454058  0.12883033\n",
      "   0.08535764]\n",
      " [ 0.12033406  0.03337482 -0.09276767 ... -0.02978439  0.06197888\n",
      "  -0.05709789]\n",
      " [ 0.01904193  0.12314246 -0.13285584 ...  0.11489463 -0.12284904\n",
      "   0.14896032]\n",
      " ...\n",
      " [-0.04400936  0.1442419  -0.14595521 ... -0.14686444 -0.09601438\n",
      "   0.12272587]\n",
      " [-0.10529449 -0.03858735 -0.00868271 ... -0.13884863  0.06846297\n",
      "  -0.02329309]\n",
      " [ 0.00580673 -0.01931079  0.02509452 ... -0.12050842 -0.13313045\n",
      "   0.11057562]] \n",
      " (100, 785)\n",
      "\n",
      "Theta2:\n",
      "  [[-0.14112536 -0.02578332  0.05575664 ...  0.04507273 -0.05993099\n",
      "   0.12177694]\n",
      " [ 0.10086767  0.01187358  0.00660116 ...  0.14632367 -0.03158312\n",
      "  -0.10983726]\n",
      " [-0.10826775 -0.03335867  0.12665115 ... -0.11684224 -0.00161223\n",
      "   0.09929614]\n",
      " ...\n",
      " [ 0.01556968  0.09470796 -0.08883631 ...  0.11569099  0.12214169\n",
      "   0.04962289]\n",
      " [-0.01684706  0.07987316 -0.13420489 ...  0.12673999 -0.07104661\n",
      "   0.11794276]\n",
      " [ 0.07839554 -0.10560515  0.02707764 ... -0.11958073  0.0922403\n",
      "  -0.04289284]] \n",
      " (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly initialise Thetas (weights) between a range of [-epsilon, epsilon]\n",
    "def initialise(a, b, epsilon):\n",
    "    # Scale and shift random values to be within range\n",
    "    c = (np.random.rand(a, b + 1) * (2 * epsilon)) - epsilon\n",
    "    \n",
    "    # Returns matrix of randomly initialised weights, of dimensions a x (b + 1)\n",
    "    return c\n",
    "\n",
    "# epsilon chosen arbitrarily (small enough to avoid saturation and large enough to avoid vanishing gradients)\n",
    "initial_Theta1 = initialise(hidden_layer_size, input_layer_size, epsilon)\n",
    "initial_Theta2 = initialise(num_labels, hidden_layer_size, epsilon)\n",
    "print(\"Theta1:\\n \", initial_Theta1, \"\\n\", initial_Theta1.shape)\n",
    "print(\"\\nTheta2:\\n \", initial_Theta2, \"\\n\", initial_Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters:  (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Unroll (combine) the weight matrices into a single column vector (easier for optimisation algorithm)\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "print(\"Initial parameters: \", initial_nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why 79,510?\n",
    "`Theta1`\n",
    "-   hidden_layer_size * (input_layer_size + 1)\n",
    "-   100 * (784 + 1)\n",
    "-   78,500\n",
    "\n",
    "`Theta2`\n",
    "-   num_labels * (hidden_layer_size + 1)\n",
    "-   10 * (100 + 1)\n",
    "-   1,010\n",
    "\n",
    "`TOTAL`\n",
    "-   79,510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of how activation functions affect the matrix: \n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Sigmoid:\n",
      " [[0.26894142 0.5        0.73105858]\n",
      " [0.88079708 0.11920292 0.95257413]]\n",
      "\n",
      "Tanh:\n",
      " [[-0.76159416  0.          0.76159416]\n",
      " [ 0.96402758 -0.96402758  0.99505475]]\n",
      "\n",
      "ReLU:\n",
      " [[0 0 1]\n",
      " [2 0 3]]\n",
      "\n",
      "Leaky ReLU:\n",
      " [[-0.01  0.    1.  ]\n",
      " [ 2.   -0.02  3.  ]]\n",
      "\n",
      "Softmax:\n",
      " [[0.09003057 0.24472847 0.66524096]\n",
      " [0.26762315 0.00490169 0.72747516]]\n",
      "\n",
      "Linear:\n",
      " [[-1  0  1]\n",
      " [ 2 -2  3]]\n",
      "\n",
      "Softplus:\n",
      " [[0.31326169 0.69314718 1.31326169]\n",
      " [2.12692801 0.12692801 3.04858735]]\n",
      "\n",
      "Hard Sigmoid:\n",
      " [[0.  0.5 1. ]\n",
      " [1.  0.  1. ]]\n",
      "\n",
      "Hard Tanh:\n",
      " [[-1  0  1]\n",
      " [ 1 -1  1]]\n",
      "\n",
      "ELU:\n",
      " [[-0.00632121  0.          1.        ]\n",
      " [ 2.         -0.00864665  3.        ]]\n",
      "\n",
      "SELU:\n",
      " [[-1.11132754  0.          1.0507    ]\n",
      " [ 2.1014     -1.52016209  3.1521    ]]\n"
     ]
    }
   ],
   "source": [
    "# Activation function for forward propagation\n",
    "def activation_function(z, function_name=\"sigmoid\", alpha=0.01):\n",
    "    # Range: [0, 1] - used for binary classification\n",
    "    if function_name == \"sigmoid\":\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Range: [-1, 1] - zero-centred, used in hidden layers\n",
    "    elif function_name == \"tanh\":\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    # Range: [0, inf) - outputs 0 for z < 0 and z for z >= 0\n",
    "    elif function_name == \"relu\":\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    # Range: (-inf, inf) - outputs z for z >= 0 and alpha * z for z < 0\n",
    "    elif function_name == \"leaky_relu\":\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    # Range: [0, 1] - used in output layers for multi-class classification\n",
    "    elif function_name == \"softmax\":\n",
    "        exp_z = np.exp(z - np.max(z))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    # Range: (-inf, inf) - linear activation\n",
    "    elif function_name == \"linear\":\n",
    "        return z\n",
    "    \n",
    "    # Range: [0, 6] - smooth approximation of ReLU\n",
    "    elif function_name == \"softplus\":\n",
    "        return np.log1p(np.exp(z))\n",
    "    \n",
    "    # Range: [-1, 1] - scaled and shifted sigmoid\n",
    "    elif function_name == \"hard_sigmoid\":\n",
    "        return np.maximum(0, np.minimum(1, (z + 1) / 2))\n",
    "    \n",
    "    # Range: (-1, 1) - similar to tanh but with a sharper bend\n",
    "    elif function_name == \"hard_tanh\":\n",
    "        return np.maximum(-1, np.minimum(1, z))\n",
    "    \n",
    "    # Range: [0, inf) - similar to ReLU but smoother near 0\n",
    "    elif function_name == \"elu\":\n",
    "        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    # Range: (-inf, inf) - scaled ReLU\n",
    "    elif function_name == \"selu\":\n",
    "        lambda_ = 1.0507\n",
    "        alpha = 1.67326\n",
    "        return lambda_ * np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {function_name}\")\n",
    "\n",
    "# Test the function\n",
    "z = np.array([[-1, 0, 1], [2, -2, 3]])\n",
    "\n",
    "print(\"Examples of how activation functions affect the matrix: \\n\", z)\n",
    "print(\"\\nSigmoid:\\n\", activation_function(z, \"sigmoid\"))\n",
    "print(\"\\nTanh:\\n\", activation_function(z, \"tanh\"))\n",
    "print(\"\\nReLU:\\n\", activation_function(z, \"relu\"))\n",
    "print(\"\\nLeaky ReLU:\\n\", activation_function(z, \"leaky_relu\"))\n",
    "print(\"\\nSoftmax:\\n\", activation_function(z, \"softmax\"))\n",
    "print(\"\\nLinear:\\n\", activation_function(z, \"linear\"))\n",
    "print(\"\\nSoftplus:\\n\", activation_function(z, \"softplus\"))\n",
    "print(\"\\nHard Sigmoid:\\n\", activation_function(z, \"hard_sigmoid\"))\n",
    "print(\"\\nHard Tanh:\\n\", activation_function(z, \"hard_tanh\"))\n",
    "print(\"\\nELU:\\n\", activation_function(z, \"elu\"))\n",
    "print(\"\\nSELU:\\n\", activation_function(z, \"selu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for cost evaluation, between true labels and predicted values\n",
    "def loss_function(y_true, y_pred, loss_type=\"cross_entropy\", epsilon=1e-12):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels (one-hot encoded or actual values).\n",
    "    y_pred (numpy array): Predicted values (probabilities or actual values).\n",
    "    loss_type (str): Name of the loss function to use.\n",
    "    epsilon (float): Small value to avoid log(0).\n",
    "    \n",
    "    Returns:\n",
    "    float: Computed loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Categorical Cross-Entropy Loss (for multi-class classification)\n",
    "    if loss_type == \"cross_entropy\":\n",
    "        # Avoids log(0) by clipping values to be within [epsilon, 1 - epsilon]\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "    # Mean Squared Error Loss (for regression or classification)\n",
    "    elif loss_type == \"mse\":\n",
    "        return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n",
    "\n",
    "    # Hinge Loss (used for binary classification with SVM-like models)\n",
    "    elif loss_type == \"hinge\":\n",
    "        return np.mean(np.maximum(0, 1 - y_true * y_pred))\n",
    "\n",
    "    # Kullback-Leibler Divergence (measures how one probability distribution diverges from another)\n",
    "    elif loss_type == \"kl_div\":\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Avoid log(0)\n",
    "        return np.sum(y_true * np.log(y_true / y_pred)) / y_true.shape[0]\n",
    "\n",
    "    # Focal Loss (addresses class imbalance by focusing more on hard examples)\n",
    "    elif loss_type == \"focal_loss\":\n",
    "        # Focusing parameter (can be adjusted)\n",
    "        gamma = 2.0\n",
    "        \n",
    "        # Avoids log(0) by clipping values to be within [epsilon, 1 - epsilon]\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y_true * (1 - y_pred) ** gamma * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "    \n",
    "    \n",
    "# Regularisation function to prevent overfitting by penalizing large weights\n",
    "def regularisation_function(Theta1, Theta2, reg_type=\"l2\", lamb=0.1):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "    Theta1 (numpy array): Weight matrix for the first layer.\n",
    "    Theta2 (numpy array): Weight matrix for the second layer.\n",
    "    regularisation_type (str): Name of the regularisation method to use.\n",
    "    lamb (float): Regularisation parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: Computed regularisation term.\n",
    "    \"\"\"\n",
    "    \n",
    "    # L2 Regularisation (penalizes large weights)\n",
    "    if reg_type == \"l2\":\n",
    "        reg_term = (lamb / 2) * (np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2))\n",
    "\n",
    "    # L1 Regularisation (penalizes the absolute value of weights)\n",
    "    elif reg_type == \"l1\":\n",
    "        reg_term = (lamb / 2) * (np.sum(np.abs(Theta1[:, 1:])) + np.sum(np.abs(Theta2[:, 1:])))\n",
    "\n",
    "    # Elastic Net Regularisation (combination of L1 and L2)\n",
    "    elif reg_type == \"elastic_net\":\n",
    "        # Mixing parameter (0 <= alpha <= 1)\n",
    "        alpha = 0.5\n",
    "        \n",
    "        # Calculate L1 and L2 regularisation terms\n",
    "        l2_term = (1 - alpha) * np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2)\n",
    "        l1_term = alpha * (np.sum(np.abs(Theta1[:, 1:])) + np.sum(np.abs(Theta2[:, 1:])))\n",
    "        reg_term = (lamb / 2) * (l2_term + l1_term)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported regularisation type: {reg_type}\")\n",
    "    \n",
    "    return reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51600032 0.68717092 0.54714208 ... 0.58938273 0.68722043 0.60409055]\n",
      " [0.50132083 0.69007448 0.5369979  ... 0.56066972 0.68547845 0.61708201]\n",
      " [0.50413639 0.72534784 0.49578934 ... 0.61509601 0.65343305 0.62850382]\n",
      " ...\n",
      " [0.5281357  0.71026649 0.48276758 ... 0.62957976 0.66973608 0.57917206]\n",
      " [0.52419096 0.67310121 0.52026561 ... 0.63028447 0.71653676 0.61308356]\n",
      " [0.51866226 0.65487893 0.51750995 ... 0.63610855 0.69637095 0.61503542]]\n"
     ]
    }
   ],
   "source": [
    "# Perform forward propagation on the training data maxtrix and weight matrices\n",
    "def forward_propagation(X, Theta1, Theta2):\n",
    "    m = X.shape[0]\n",
    "    ones = np.ones((m, 1))                  # Ones matrix for bias unit (60,000 x 1)\n",
    "    X = np.append(ones, X, axis=1)          # Add bias unit to input layer (60,000 x 785)\n",
    "    a1 = X                                  # Input layer\n",
    "    z2 = np.dot(a1, Theta1.T)               # Compute z2 for hidden layer (60,000 x 100)\n",
    "    a2 = activation_function(z2, 'sigmoid')\n",
    "    a2 = np.append(ones, a2, axis=1)        # Add bias unit to hidden layer (60,000 x 101)\n",
    "    z3 = np.dot(a2, Theta2.T)               # Compute z3 for output layer (60,000 x 10)\n",
    "    a3 = activation_function(z3, 'sigmoid')\n",
    "    \n",
    "    return a1, z2, a2, z3, a3\n",
    "\n",
    "# Calculate the cost function\n",
    "def cost_function(a3, y, Theta1, Theta2, lamb, loss_type=\"cross_entropy\", reg_type=\"l2\"):\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors (one-hot encoding)\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Calculate the cost (loss + regularisation)\n",
    "    loss = loss_function(y_vect, a3, loss_type=loss_type)\n",
    "    reg_term = regularisation_function(Theta1, Theta2, reg_type=reg_type, lamb=lamb)\n",
    "    J = loss + (reg_term / m)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Perform back propagation on the data matrix and weight matrices\n",
    "def back_propagation(a1, z2, a2, a3, y, Theta1, Theta2):\n",
    "    m = y.shape[0]\n",
    "    num_labels = a3.shape[1]\n",
    "    \n",
    "    # Convert y labels into binary vectors\n",
    "    y_vect = np.zeros((m, num_labels))          # Zeros 60,000 x 10\n",
    "    for i in range(m):                          # For each row in y\n",
    "        y_vect[i, int(y[i])] = 1                # Set the value at the index of the label to 1 (e.g. 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    # Compute deltas\n",
    "    Delta3 = a3 - y_vect\n",
    "    Delta2 = np.dot(Delta3, Theta2) * a2 * (1 - a2)\n",
    "    Delta2 = Delta2[:, 1:]  # Remove bias term from Delta2\n",
    "    \n",
    "    return Delta3, Delta2\n",
    "\n",
    "# Compute the gradients of the cost function\n",
    "def compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb):\n",
    "    m = a1.shape[0]\n",
    "    \n",
    "    # Set the first column of Theta1 and Theta2 to zero (bias terms are not regularized)\n",
    "    Theta1[:, 0] = 0\n",
    "    Theta2[:, 0] = 0\n",
    "    \n",
    "    # Compute the gradients\n",
    "    Theta1_grad = (1 / m) * np.dot(Delta2.T, a1) + (lamb / m) * Theta1\n",
    "    Theta2_grad = (1 / m) * np.dot(Delta3.T, a2) + (lamb / m) * Theta2\n",
    "    \n",
    "    # Flatten gradients to return a single vector\n",
    "    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "def neural_network(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    # Unroll the parameters\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))\n",
    "    \n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, Theta1, Theta2)\n",
    "    \n",
    "    # Cost function\n",
    "    J = cost_function(a3, y, Theta1, Theta2, lamb)\n",
    "    \n",
    "    # Back propagation\n",
    "    Delta3, Delta2 = back_propagation(a1, z2, a2, a3, y, Theta1, Theta2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad = compute_gradients(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb)\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.minimize`\n",
    "Generic optimisation function to find the minimum of a scalar function\n",
    "-   Objective Function: cost function of NN\n",
    "-   Initial Guess: initial estimate of params (weights) to optimise\n",
    "-   Method: optimisation algorithm to be used\n",
    "-   Jac: Jacobian (gradient) of objective function\n",
    "-   Options: e.g. number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        79510     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.67378D+00    |proj g|=  5.15862D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  3.35710D+00    |proj g|=  5.89480D-02\n",
      "\n",
      "At iterate    2    f=  3.25141D+00    |proj g|=  3.61778D-02\n",
      "\n",
      "At iterate    3    f=  3.20114D+00    |proj g|=  2.49376D-02\n",
      "\n",
      "At iterate    4    f=  3.15153D+00    |proj g|=  3.00432D-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m myargs \u001b[38;5;241m=\u001b[39m (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Minimise the cost function using the L-BFGS-B optimisation algorithm\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_nn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:343\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Documents/Coding/Neural Network/neural-network/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[33], line 87\u001b[0m, in \u001b[0;36mneural_network\u001b[0;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb)\u001b[0m\n\u001b[1;32m     84\u001b[0m Delta3, Delta2 \u001b[38;5;241m=\u001b[39m back_propagation(a1, z2, a2, a3, y, Theta1, Theta2)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDelta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDelta3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m J, grad\n",
      "Cell \u001b[0;32mIn[33], line 62\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(Delta2, Delta3, a1, a2, Theta1, Theta2, lamb)\u001b[0m\n\u001b[1;32m     59\u001b[0m Theta2[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Compute the gradients\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m Theta1_grad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDelta2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (lamb \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m Theta1\n\u001b[1;32m     63\u001b[0m Theta2_grad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Delta3\u001b[38;5;241m.\u001b[39mT, a2) \u001b[38;5;241m+\u001b[39m (lamb \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m Theta2\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Flatten gradients to return a single vector\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Package all arguments into a tuple for optimisation function\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n",
    "\n",
    "# Minimise the cost function using the L-BFGS-B optimisation algorithm\n",
    "results = minimize(neural_network, x0=initial_nn_params, args=myargs, options={'disp': True, 'maxiter': maxiter}, method='L-BFGS-B', jac=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Objective / Cost function`\n",
    "| Variable | Definition                                                                     |\n",
    "| -------- | -------                                                                        |\n",
    "| N        | total number of variables (parameters, weights)                                |\n",
    "| M        | number of corrections (updates) stored in memory by the algorithm              |\n",
    "| X0       | none of the variables are at their bounds                                      |\n",
    "| f        | current value of cost function                                                 |\n",
    "| proj g   | norm of projected gradient (how close optimisation is to a stationary point)   |\n",
    "| success  | whether optimisation converged or not (did not)                                |\n",
    "| fun      | value of cost function @ final params                                          |\n",
    "| x        | final values of params (weights)                                               |\n",
    "| nit      | number of iterations performed                                                 |\n",
    "| jac      | gradient of cost function @ final params                                       |\n",
    "| nfev     | # times cost function was evaluated                                            |\n",
    "| njev     | # times gradient was evaluated                                                 |\n",
    "| hess_inv | inverse of the approx Hessian matrix @ final point (helps understand curvature)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised weights: \n",
      " [-0.58609539  0.13218445  0.11142981 ... -2.93568529  0.56368882\n",
      "  0.02294904] \n",
      " (79510,)\n"
     ]
    }
   ],
   "source": [
    "# Extract the optimised weights (trained Theta)\n",
    "nn_params = results[\"x\"]\n",
    "print(\"Optimised weights: \\n\", nn_params, \"\\n\", nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 785) (10, 101)\n"
     ]
    }
   ],
   "source": [
    "# Split weights back into Theta1 (100 x 785) and Theta2 (10 x 101)\n",
    "Theta1 = np.reshape(nn_params[:(hidden_layer_size * (input_layer_size + 1))], (hidden_layer_size, (input_layer_size + 1)))\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):], (num_labels, (hidden_layer_size + 1)))\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  97.28\n",
      "Train set accuracy:  99.50333333333333\n"
     ]
    }
   ],
   "source": [
    "# Performs forward propagation to predict the label (digit) of an input image\n",
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0] \n",
    "    ones = np.ones((m, 1))                      # Ones matrix\n",
    "    X = np.append(ones, X, axis=1)              # Add bias unit to input (first) layer \n",
    "    z2 = np.dot(X, Theta1.transpose()) \n",
    "    a2 = 1 / (1 + np.exp(-z2))                  # Activation for hidden (second) layer\n",
    "    ones = np.ones((m, 1)) \n",
    "    a2 = np.append(ones, a2, axis=1)            # Adding bias unit to hidden layer \n",
    "    z3 = np.dot(a2, Theta2.transpose()) \n",
    "    a3 = 1 / (1 + np.exp(-z3))                  # Activation for output (third) layer \n",
    "    p = (np.argmax(a3, axis=1))                 # Predicting the class on the basis of max value of hypothesis \n",
    "    return p \n",
    "\n",
    "\n",
    "# Check test set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_test)\n",
    "print(\"Test set accuracy: \", np.mean(pred == y_test) * 100)\n",
    "\n",
    "# Check train set accuracy of model\n",
    "pred = predict(Theta1, Theta2, X_train)\n",
    "print(\"Train set accuracy: \", np.mean(pred == y_train) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9950333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate precision of model \n",
    "true_positive = 0\n",
    "\n",
    "# Iterate through predictions and compare with actual labels\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == y_train[i]:\n",
    "        true_positive += 1\n",
    "        \n",
    "# Calculate precision\n",
    "false_positive = len(y_train) - true_positive \n",
    "print('Precision =', true_positive/(true_positive + false_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Thetas in .txt files: (100 x 785) and (10 x 101)\n",
    "np.savetxt('Theta1.txt', Theta1, delimiter=' ') \n",
    "np.savetxt('Theta2.txt', Theta2, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop up widget to draw digit and output model prediction\n",
    "window = Tk() \n",
    "window.title(\"Handwritten digit recognition\") \n",
    "l1 = Label() \n",
    "\n",
    "\n",
    "def MyProject():\n",
    "\tglobal l1 \n",
    "\n",
    "\twidget = cv \n",
    "\t# Setting co-ordinates of canvas\n",
    "\tx = window.winfo_rootx() + widget.winfo_x() \n",
    "\ty = window.winfo_rooty() + widget.winfo_y() \n",
    "\tx1 = x + widget.winfo_width() \n",
    "\ty1 = y + widget.winfo_height() \n",
    "\n",
    "\t# Image is captured from canvas and is resized to (28 X 28) px \n",
    "\timg = ImageGrab.grab().crop((x, y, x1, y1)).resize((28, 28)) \n",
    "\n",
    "\t# Converting rgb to grayscale image \n",
    "\timg = img.convert('L') \n",
    "\n",
    "\t# Extracting pixel matrix of image and converting it to a vector of (1, 784) \n",
    "\tx = np.asarray(img) \n",
    "\tvec = np.zeros((1, 784)) \n",
    "\tk = 0\n",
    "\tfor i in range(28): \n",
    "\t\tfor j in range(28): \n",
    "\t\t\tvec[0][k] = x[i][j] \n",
    "\t\t\tk += 1\n",
    "\n",
    "\t# Loading Thetas \n",
    "\tTheta1 = np.loadtxt('Theta1.txt') \n",
    "\tTheta2 = np.loadtxt('Theta2.txt')\n",
    "\n",
    "\t# Calling function for prediction \n",
    "\tpred = predict(Theta1, Theta2, vec / 255)\n",
    "\n",
    "\t# Displaying the result \n",
    "\tl1 = Label(window, text=\"Digit = \" + str(pred[0]), font=('Algerian', 20)) \n",
    "\tl1.place(x=230, y=420) \n",
    "\n",
    "\n",
    "lastx, lasty = None, None\n",
    "\n",
    "\n",
    "# Clears the canvas \n",
    "def clear_widget(): \n",
    "\tglobal cv, l1 \n",
    "\tcv.delete(\"all\") \n",
    "\tl1.destroy() \n",
    "\n",
    "\n",
    "# Activate canvas\n",
    "def event_activation(event): \n",
    "\tglobal lastx, lasty \n",
    "\tcv.bind('<B1-Motion>', draw_lines) \n",
    "\tlastx, lasty = event.x, event.y \n",
    "\n",
    "\n",
    "# To draw on canvas \n",
    "def draw_lines(event): \n",
    "\tglobal lastx, lasty \n",
    "\tx, y = event.x, event.y \n",
    "\tcv.create_line((lastx, lasty, x, y), width=30, fill='white', capstyle=ROUND, smooth=TRUE, splinesteps=12) \n",
    "\tlastx, lasty = x, y \n",
    "\n",
    "\n",
    "# Label \n",
    "L1 = Label(window, text=\"Handwritten Digit Recoginition\", font=('Algerian', 25), fg=\"blue\") \n",
    "L1.place(x=35, y=10) \n",
    "\n",
    "# Button to clear canvas \n",
    "b1 = Button(window, text=\"1. Clear Canvas\", font=('Algerian', 15), bg=\"orange\", fg=\"black\", command=clear_widget) \n",
    "b1.place(x=120, y=370) \n",
    "\n",
    "# Button to predict digit drawn on canvas \n",
    "b2 = Button(window, text=\"2. Prediction\", font=('Algerian', 15), bg=\"white\", fg=\"red\", command=MyProject) \n",
    "b2.place(x=320, y=370) \n",
    "\n",
    "# Setting properties of canvas \n",
    "cv = Canvas(window, width=350, height=290, bg='black') \n",
    "cv.place(x=120, y=70) \n",
    "\n",
    "cv.bind('<Button-1>', event_activation) \n",
    "window.geometry(\"600x500\") \n",
    "window.mainloop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
